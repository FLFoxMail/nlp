{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªåŠ¨å¾®åˆ†\n",
    "\n",
    "è®¡ç®—æœºæ±‚å¯¼æ–¹æ³•\n",
    "å¯¹è®¡ç®—æœºç¨‹åºæ±‚å¯¼çš„æ–¹æ³•å¯ä»¥å½’çº³ä¸ºä»¥ä¸‹å››ç§ï¼š\n",
    "\n",
    "**æ‰‹åŠ¨æ±‚è§£æ³•(Manual Differentiation) **ï¼šæ ¹æ®é“¾å¼æ±‚å¯¼æ³•åˆ™ï¼Œæ‰‹å·¥æ±‚å¯¼å¹¶ç¼–å†™å¯¹åº”çš„ç»“æœç¨‹åºï¼Œä¾æ®é“¾å¼æ³•åˆ™è§£å‡ºæ¢¯åº¦å…¬å¼ï¼Œå¸¦å…¥æ•°å€¼ï¼Œå¾—åˆ°æ¢¯åº¦ã€‚\n",
    "\n",
    "æ•°å€¼å¾®åˆ†æ³•(Numerical Differentiation)ï¼šåˆ©ç”¨å¯¼æ•°çš„åŸå§‹å®šä¹‰ï¼Œé€šè¿‡æœ‰é™å·®åˆ†è¿‘ä¼¼æ–¹æ³•å®Œæˆæ±‚å¯¼ï¼Œç›´æ¥æ±‚è§£å¾®åˆ†å€¼ã€‚\n",
    "\n",
    "ç¬¦å·å¾®åˆ†æ³•(Symbolic Differentiation)ï¼šåŸºäºæ•°å­¦è§„åˆ™å’Œç¨‹åºè¡¨è¾¾å¼å˜æ¢å®Œæˆæ±‚å¯¼ã€‚åˆ©ç”¨æ±‚å¯¼è§„åˆ™å¯¹è¡¨è¾¾å¼è¿›è¡Œè‡ªåŠ¨è®¡ç®—ï¼Œå…¶è®¡ç®—ç»“æœæ˜¯å¯¼å‡½æ•°çš„è¡¨è¾¾å¼è€Œéå…·ä½“çš„æ•°å€¼ã€‚å³ï¼Œå…ˆæ±‚è§£æè§£ï¼Œç„¶åè½¬æ¢ä¸ºç¨‹åºï¼Œå†é€šè¿‡ç¨‹åºè®¡ç®—å‡ºå‡½æ•°çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "è‡ªåŠ¨å¾®åˆ†æ³•(Automatic Differentiation)ï¼šä»‹äºæ•°å€¼å¾®åˆ†å’Œç¬¦å·å¾®åˆ†ä¹‹é—´çš„æ–¹æ³•ï¼Œé‡‡ç”¨ç±»ä¼¼æœ‰å‘å›¾çš„è®¡ç®—æ¥æ±‚è§£å¾®åˆ†å€¼ï¼Œä¹Ÿæ˜¯æœ¬æ–‡ä»‹ç»çš„é‡ç‚¹ã€‚\n",
    "\n",
    "è€Œè‡ªåŠ¨å¾®åˆ†åˆ™æ˜¯åˆ†ä¸ºå‰å‘å¾®åˆ†å’Œåå‘å¾®åˆ†ä¸¤ç§å®ç°æ¨¡å¼ï¼Œä¸åŒçš„å®ç°æ¨¡å¼æœ‰ä¸åŒçš„æœºåˆ¶å’Œè®¡ç®—é€»è¾‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é›…å„æ¯”çŸ©é˜µ\n",
    "\n",
    "åœ¨å‘é‡å¾®ç§¯åˆ†ä¸­ï¼Œ**Jacobian çŸ©é˜µæ˜¯**ä¸€é˜¶åå¯¼æ•°ä»¥ä¸€å®šæ–¹å¼æ’åˆ—æˆçš„çŸ©é˜µï¼Œå…¶è¡Œåˆ—å¼ç§°ä¸º Jacobian è¡Œåˆ—å¼ã€‚Jacobian çŸ©é˜µçš„é‡è¦æ€§åœ¨äºå®ƒä½“ç°äº†ä¸€ä¸ªå¯å¾®æ–¹ç¨‹ä¸ç»™å‡ºç‚¹çš„æœ€ä¼˜çº¿æ€§é€¼è¿‘ã€‚\n",
    "\n",
    "Jacobian çŸ©é˜µè¡¨ç¤ºä¸¤ä¸ªå‘é‡æ‰€æœ‰å¯èƒ½çš„åå¯¼æ•°ã€‚å®ƒæ˜¯ä¸€ä¸ªå‘é‡ç›¸å¯¹äºå¦ä¸€ä¸ªå‘é‡çš„æ¢¯åº¦ï¼Œå…¶å®ç°çš„æ˜¯ $n$ ç»´å‘é‡åˆ° $m$ ç»´å‘é‡çš„æ˜ å°„ã€‚\n",
    "\n",
    "åœ¨çŸ¢é‡è¿ç®—ä¸­ï¼ŒJacobian çŸ©é˜µæ˜¯åŸºäºå‡½æ•°å¯¹æ‰€æœ‰å˜é‡ä¸€é˜¶åå¯¼æ•°çš„æ•°å€¼çŸ©é˜µï¼Œå½“è¾“å…¥ä¸ªæ•°ç­‰äºè¾“å‡ºä¸ªæ•°æ—¶åˆç§°ä¸º **Jacobian** è¡Œåˆ—å¼ã€‚\n",
    "\n",
    "å‡è®¾è¾“å…¥å‘é‡ $ğ‘¥âˆˆğ‘…_n$ï¼Œè€Œè¾“å‡ºå‘é‡ $ğ‘¦âˆˆğ‘…_m$ï¼Œåˆ™ Jacobian çŸ©é˜µå®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "J_f= \\left[ \\begin{matrix} \\dfrac{\\delta y_1}{\\delta x_1} & \\cdots & \\dfrac{\\delta y_1}{\\delta x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\dfrac{\\delta y_m}{\\delta x_1} & \\vdots & \\dfrac{\\delta y_m}{\\delta x_n} \\end{matrix} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‡ªåŠ¨å¾®åˆ†ç†è®ºè¡¥å……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/chenzomi12/AISystem/blob/main/05Framework/02AutoDiff/03GradMode.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªåŠ¨å¾®åˆ†å®ç°\n",
    "é¢„å®šä¹‰äº†ç‰¹å®šçš„æ•°æ®ç»“æ„ï¼Œå¹¶å¯¹è¯¥æ•°æ®ç»“æ„é‡è½½äº†ç›¸åº”çš„åŸºæœ¬è¿ç®—æ“ä½œç¬¦ï¼›\n",
    "ç¨‹åºåœ¨å®é™…æ‰§è¡Œæ—¶ä¼šå°†ç›¸åº”è¡¨è¾¾å¼çš„æ“ä½œç±»å‹å’Œè¾“å…¥è¾“å‡ºä¿¡æ¯è®°å½•è‡³ç‰¹æ®Šæ•°æ®ç»“æ„ï¼›\n",
    "å¾—åˆ°ç‰¹æ®Šæ•°æ®ç»“æ„åï¼Œå°†å¯¹æ•°æ®ç»“æ„è¿›è¡Œéå†å¹¶å¯¹å…¶ä¸­è®°å½•çš„åŸºæœ¬è¿ç®—æ“ä½œè¿›è¡Œå¾®åˆ†ï¼›\n",
    "æŠŠç»“æœé€šè¿‡é“¾å¼æ³•åˆ™è¿›è¡Œç»„åˆï¼Œå®Œæˆè‡ªåŠ¨å¾®åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.å‰å‘å¾®åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰å‘è‡ªåŠ¨å¾®åˆ†å…³é”®æ­¥éª¤ä¸ºï¼š\n",
    "\n",
    "åˆ†è§£ç¨‹åºä¸ºä¸€ç³»åˆ—å·²çŸ¥å¾®åˆ†è§„åˆ™çš„åŸºç¡€è¡¨è¾¾å¼çš„ç»„åˆ\n",
    "æ ¹æ®å·²çŸ¥å¾®åˆ†è§„åˆ™ç»™å‡ºå„åŸºç¡€è¡¨è¾¾å¼çš„å¾®åˆ†ç»“æœ\n",
    "æ ¹æ®åŸºç¡€è¡¨è¾¾å¼é—´çš„æ•°æ®ä¾èµ–å…³ç³»ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™å°†å¾®åˆ†ç»“æœç»„åˆå®Œæˆç¨‹åºçš„å¾®åˆ†ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ADTangent:\n",
    "    # è‡ªå˜é‡ x å’Œ å¯¼æ•° dx\n",
    "    def __init__(self, x, dx):\n",
    "        self.x = x\n",
    "        self.dx = dx\n",
    "    \n",
    "    def __str__(self):\n",
    "        context = f'value: {self.x}, grad: {self.dx}'\n",
    "        return context\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # åˆ¤æ–­æ“ä½œæ•°æ˜¯å¦ä¸º ADTangent ç±»å‹\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x + other.x\n",
    "            dx = self.dx + other.dx\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x + other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for +: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x - other.x\n",
    "            dx = self.dx - other.dx\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x - other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for -: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x * other.x\n",
    "            dx = self.x * other.dx + self.dx * other.x\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x * other\n",
    "            dx = self.dx * other\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for *: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    \n",
    "    def log(self):\n",
    "        x = np.log(self.x)\n",
    "        dx = self.dx / self.x\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def sin(self):\n",
    "        x = np.sin(self.x)\n",
    "        dx = self.dx * np.cos(self.x)   \n",
    "        return ADTangent(x, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 9.734222905896807, grad: 5.5\n"
     ]
    }
   ],
   "source": [
    "x = ADTangent(x = 2., dx = 1)\n",
    "y = ADTangent(x = 5., dx = 0)\n",
    "\n",
    "f = ADTangent.log(x) + x * y + ADTangent.sin(y)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5000) tensor(2.2837)\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ torch éªŒè¯\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.tensor(2.0), requires_grad=True)\n",
    "y = Variable(torch.tensor(5.0), requires_grad=True)\n",
    "z = torch.log(x) + x * y + torch.sin(y)\n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.åå‘å¾®åˆ†\n",
    "åå‘è‡ªåŠ¨å¾®åˆ†å…³é”®æ­¥éª¤ä¸ºï¼š\n",
    "åå‘æ¨¡å¼æ ¹æ®ä»åå‘å‰è®¡ç®—ï¼Œä¾æ¬¡å¾—åˆ°å¯¹æ¯ä¸ªä¸­é—´å˜é‡èŠ‚ç‚¹çš„åå¯¼æ•°ï¼Œç›´åˆ°åˆ°è¾¾è‡ªå˜é‡èŠ‚ç‚¹å¤„ï¼Œè¿™æ ·å°±å¾—åˆ°äº†æ¯ä¸ªè¾“å…¥çš„åå¯¼æ•°ã€‚åœ¨æ¯ä¸ªèŠ‚ç‚¹å¤„ï¼Œæ ¹æ®è¯¥èŠ‚ç‚¹çš„åç»­èŠ‚ç‚¹ï¼ˆå‰å‘ä¼ æ’­ä¸­çš„åç»­èŠ‚ç‚¹ï¼‰è®¡ç®—å…¶å¯¼æ•°å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple, Callable, Dict, Optional\n",
    "\n",
    "_name = 1\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºç”Ÿæˆæ–°çš„åå­—\n",
    "def fresh_name():\n",
    "    # å£°æ˜ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨å½“å‰çš„åå­—\n",
    "    global _name\n",
    "    # ç”Ÿæˆæ–°çš„åå­—ï¼Œæ ¼å¼ä¸ºvåŠ ä¸Šå½“å‰çš„åå­—\n",
    "    name = f'v{_name}'\n",
    "    # å°†å½“å‰çš„åå­—åŠ 1\n",
    "    _name += 1\n",
    "    # è¿”å›æ–°çš„åå­—\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Variable:\n",
    "    # åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªå¯¹è±¡\n",
    "    def __init__(self, value, name=None):\n",
    "        # å°†ä¼ å…¥çš„valueå‚æ•°èµ‹å€¼ç»™å¯¹è±¡çš„valueå±æ€§\n",
    "        self.value = value\n",
    "        # å¦‚æœä¼ å…¥çš„nameå‚æ•°ä¸ºç©ºï¼Œåˆ™è°ƒç”¨fresh_name()å‡½æ•°ç”Ÿæˆä¸€ä¸ªæ–°çš„åå­—ï¼Œå¦åˆ™å°†ä¼ å…¥çš„nameå‚æ•°èµ‹å€¼ç»™å¯¹è±¡çš„nameå±æ€§\n",
    "        self.name = name or fresh_name()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # è¿”å›self.valueçš„å­—ç¬¦ä¸²è¡¨ç¤º\n",
    "        return repr(self.value)\n",
    "\n",
    "    @staticmethod\n",
    "    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªå¸¸é‡\n",
    "    def constant(value, name=None):\n",
    "        # åˆ›å»ºä¸€ä¸ªå˜é‡å¯¹è±¡ï¼Œä¼ å…¥å€¼å’Œåç§°\n",
    "        var = Variable(value, name)\n",
    "        # æ‰“å°å˜é‡çš„åç§°å’Œå€¼\n",
    "        print(f'{var.name} = {var.value}')\n",
    "        # è¿”å›å˜é‡å¯¹è±¡\n",
    "        return var\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return ops_mul(self, other)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return ops_add(self, other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return ops_sub(self, other)\n",
    "\n",
    "    def sin(self):\n",
    "        return ops_sin(self)\n",
    "    \n",
    "    def log(self):\n",
    "        return ops_log(self)\n",
    "\n",
    "def ops_mul(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value * other.value)\n",
    "    print(f'{x.name} = {self.name} * {other.name}')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        # è®¡ç®—æ¢¯åº¦\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dother = self # r=self.value * other.value\n",
    "        dx_self = other # dx_dother = self.value * other.value\n",
    "        dl_dself = dl_dx * dx_self\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        dl_inputs = [dl_dself, dl_dother]\n",
    "        return dl_inputs\n",
    "    # è®°å½•æ“ä½œçš„è¾“å…¥è¾“å‡º\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_add(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value + other.value)\n",
    "    print(f'{x.name} = {self.name} + {other.name}')\n",
    "\n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        # è®¡ç®—æ¢¯åº¦\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        return [dl_dself, dl_dother]\n",
    "    # è®°å½•æ“ä½œçš„è¾“å…¥è¾“å‡º\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_sub(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value - other.value)\n",
    "    print(f'{x.name} = {self.name} - {other.name}')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(-1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        return [dl_dself, dl_dother]\n",
    "    # è®°å½•æ“ä½œçš„è¾“å…¥è¾“å‡º\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_sin(self):\n",
    "    # forward\n",
    "    x = Variable(np.sin(self.value))\n",
    "    print(f'{x.name} = sin({self.name})')\n",
    "\n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(np.cos(self.value))\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        return [dl_dself]\n",
    "    \n",
    "    # è®°å½•æ“ä½œçš„è¾“å…¥è¾“å‡º\n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_log(self):\n",
    "    # forward\n",
    "    x = Variable(np.log(self.value))\n",
    "    print(f'{x.name} = log({self.name})')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0 / self.value)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        return [dl_dself]\n",
    "    # è®°å½•æ“ä½œçš„è¾“å…¥è¾“å‡º\n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# è¿™ä¸ªç±»çš„ä½œç”¨åœ¨äºè®°å½•å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸­é—´å˜é‡ï¼Œä»¥åŠè¿™äº›ä¸­é—´å˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»¥ä¾¿äºåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—æ¢¯åº¦ã€‚\n",
    "# NamedTuple æ˜¯ä¸€ä¸ªå…ƒç»„å­ç±»ï¼Œç”¨äºå®šä¹‰ä¸å¯å˜çš„æ•°æ®ç»“æ„ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ªå±æ€§ï¼šinputsã€outputs å’Œ propagateã€‚\n",
    "class Tape(NamedTuple):\n",
    "    # è¾“å…¥åˆ—è¡¨\n",
    "    inputs: List[str]\n",
    "    # è¾“å‡ºåˆ—è¡¨\n",
    "    outputs: List[str]\n",
    "    # ä¼ æ’­å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå˜é‡åˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªå˜é‡åˆ—è¡¨\n",
    "    propagate : 'Callable[List[Variable], List[Variable]]'\n",
    "\n",
    "# é‡ç½® Tape çš„æ–¹æ³• reset_tapeï¼Œæ–¹ä¾¿è¿è¡Œå¤šæ¬¡è‡ªåŠ¨å¾®åˆ†ï¼Œæ¯æ¬¡è‡ªåŠ¨å¾®åˆ†è¿‡ç¨‹éƒ½ä¼šäº§ç”Ÿ Tape List\n",
    "gradient_tape : List[Tape] = []\n",
    "\n",
    "def reset_tape():\n",
    "    global _name\n",
    "    _name = 1\n",
    "    gradient_tape.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 = log(v-1)\n",
      "v2 = v-1 * v0\n",
      "v3 = v1 + v2\n",
      "v4 = sin(v0)\n",
      "v5 = v3 - v4\n",
      "np.float64(11.652071455223084)\n"
     ]
    }
   ],
   "source": [
    "def gred(l, results):\n",
    "    dl_d = {} # dl/dX çš„æ‰€æœ‰æ¢¯åº¦\n",
    "    dl_d[l.name] = Variable(1.)\n",
    "\n",
    "    print(\"dl_d\", dl_d)\n",
    "    # æ·»åŠ  dl/dl = 1\n",
    "    \n",
    "    def gather_grads(entrys):\n",
    "        return [dl_d[entry] if entry in dl_d else None for entry in entrys]\n",
    "    \n",
    "\n",
    "    # åå‘ä¼ æ’­\n",
    "    for entry in reversed(gradient_tape):\n",
    "        # è®¡ç®—æ¢¯åº¦\n",
    "        print(entry)\n",
    "        dl_outputs = gather_grads(entry.outputs)\n",
    "        dl_inputs = entry.propagate(dl_outputs)\n",
    "        \n",
    "        for input, grad in zip(entry.inputs, dl_inputs):\n",
    "            if input not in dl_d:\n",
    "                dl_d[input] = grad\n",
    "            else:\n",
    "                dl_d[input] = dl_d[input] + grad\n",
    "                \n",
    "    for name, grad in dl_d.items():\n",
    "        print(f'd{l.name}_d{name} = {grad.value}')\n",
    "    \n",
    "    return gather_grads([result.name for result in results])\n",
    "\n",
    "reset_tape()\n",
    "x = Variable(2.0, name='v-1')\n",
    "y = Variable(5.0, name='v0')\n",
    "\n",
    "f = Variable.log(x) + x * y - Variable.sin(y)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_d {'v5': 1.0}\n",
      "Tape(inputs=['v3', 'v4'], outputs=['v5'], propagate=<function ops_sub.<locals>.propagate at 0x000001EC37302EE0>)\n",
      "v9 = v6 * v7\n",
      "v10 = v6 * v8\n",
      "Tape(inputs=['v0'], outputs=['v4'], propagate=<function ops_sin.<locals>.propagate at 0x000001EC37302790>)\n",
      "v12 = v10 * v11\n",
      "Tape(inputs=['v1', 'v2'], outputs=['v3'], propagate=<function ops_add.<locals>.propagate at 0x000001EC25759940>)\n",
      "v15 = v9 * v13\n",
      "v16 = v9 * v14\n",
      "Tape(inputs=['v-1', 'v0'], outputs=['v2'], propagate=<function ops_mul.<locals>.propagate at 0x000001EC372D4DC0>)\n",
      "v17 = v16 * v0\n",
      "v18 = v16 * v-1\n",
      "v19 = v12 + v18\n",
      "Tape(inputs=['v-1'], outputs=['v1'], propagate=<function ops_log.<locals>.propagate at 0x000001EC372D2EE0>)\n",
      "v21 = v15 * v20\n",
      "v22 = v17 + v21\n",
      "dv5_dv5 = 1.0\n",
      "dv5_dv3 = 1.0\n",
      "dv5_dv4 = -1.0\n",
      "dv5_dv0 = 1.7163378145367738\n",
      "dv5_dv1 = 1.0\n",
      "dv5_dv2 = 1.0\n",
      "dv5_dv-1 = 5.5\n",
      "5.5\n",
      "np.float64(1.7163378145367738)\n"
     ]
    }
   ],
   "source": [
    "dx, dy = gred(f, [x, y])\n",
    "print(dx)\n",
    "print(dy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
