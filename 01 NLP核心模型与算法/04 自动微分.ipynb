{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动微分\n",
    "\n",
    "计算机求导方法\n",
    "对计算机程序求导的方法可以归纳为以下四种：\n",
    "\n",
    "**手动求解法(Manual Differentiation) **：根据链式求导法则，手工求导并编写对应的结果程序，依据链式法则解出梯度公式，带入数值，得到梯度。\n",
    "\n",
    "数值微分法(Numerical Differentiation)：利用导数的原始定义，通过有限差分近似方法完成求导，直接求解微分值。\n",
    "\n",
    "符号微分法(Symbolic Differentiation)：基于数学规则和程序表达式变换完成求导。利用求导规则对表达式进行自动计算，其计算结果是导函数的表达式而非具体的数值。即，先求解析解，然后转换为程序，再通过程序计算出函数的梯度。\n",
    "\n",
    "自动微分法(Automatic Differentiation)：介于数值微分和符号微分之间的方法，采用类似有向图的计算来求解微分值，也是本文介绍的重点。\n",
    "\n",
    "而自动微分则是分为前向微分和后向微分两种实现模式，不同的实现模式有不同的机制和计算逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 雅各比矩阵\n",
    "\n",
    "在向量微积分中，**Jacobian 矩阵是**一阶偏导数以一定方式排列成的矩阵，其行列式称为 Jacobian 行列式。Jacobian 矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近。\n",
    "\n",
    "Jacobian 矩阵表示两个向量所有可能的偏导数。它是一个向量相对于另一个向量的梯度，其实现的是 $n$ 维向量到 $m$ 维向量的映射。\n",
    "\n",
    "在矢量运算中，Jacobian 矩阵是基于函数对所有变量一阶偏导数的数值矩阵，当输入个数等于输出个数时又称为 **Jacobian** 行列式。\n",
    "\n",
    "假设输入向量 $𝑥∈𝑅_n$，而输出向量 $𝑦∈𝑅_m$，则 Jacobian 矩阵定义为：\n",
    "\n",
    "$$\n",
    "J_f= \\left[ \\begin{matrix} \\dfrac{\\delta y_1}{\\delta x_1} & \\cdots & \\dfrac{\\delta y_1}{\\delta x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\dfrac{\\delta y_m}{\\delta x_1} & \\vdots & \\dfrac{\\delta y_m}{\\delta x_n} \\end{matrix} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动微分理论补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/chenzomi12/AISystem/blob/main/05Framework/02AutoDiff/03GradMode.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动微分实现\n",
    "预定义了特定的数据结构，并对该数据结构重载了相应的基本运算操作符；\n",
    "程序在实际执行时会将相应表达式的操作类型和输入输出信息记录至特殊数据结构；\n",
    "得到特殊数据结构后，将对数据结构进行遍历并对其中记录的基本运算操作进行微分；\n",
    "把结果通过链式法则进行组合，完成自动微分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.前向微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向自动微分关键步骤为：\n",
    "\n",
    "分解程序为一系列已知微分规则的基础表达式的组合\n",
    "根据已知微分规则给出各基础表达式的微分结果\n",
    "根据基础表达式间的数据依赖关系，使用链式法则将微分结果组合完成程序的微分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ADTangent:\n",
    "    # 自变量 x 和 导数 dx\n",
    "    def __init__(self, x, dx):\n",
    "        self.x = x\n",
    "        self.dx = dx\n",
    "    \n",
    "    def __str__(self):\n",
    "        context = f'value: {self.x}, grad: {self.dx}'\n",
    "        return context\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # 判断操作数是否为 ADTangent 类型\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x + other.x\n",
    "            dx = self.dx + other.dx\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x + other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for +: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x - other.x\n",
    "            dx = self.dx - other.dx\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x - other\n",
    "            dx = self.dx\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for -: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ADTangent):\n",
    "            x = self.x * other.x\n",
    "            dx = self.x * other.dx + self.dx * other.x\n",
    "        elif isinstance(other, (int, float)):\n",
    "            x = self.x * other\n",
    "            dx = self.dx * other\n",
    "        else:\n",
    "            raise TypeError('unsupported operand type(s) for *: %s and %s' % (type(self), type(other)))\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    \n",
    "    def log(self):\n",
    "        x = np.log(self.x)\n",
    "        dx = self.dx / self.x\n",
    "        return ADTangent(x, dx)\n",
    "    \n",
    "    def sin(self):\n",
    "        x = np.sin(self.x)\n",
    "        dx = self.dx * np.cos(self.x)   \n",
    "        return ADTangent(x, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 9.734222905896807, grad: 5.5\n"
     ]
    }
   ],
   "source": [
    "x = ADTangent(x = 2., dx = 1)\n",
    "y = ADTangent(x = 5., dx = 0)\n",
    "\n",
    "f = ADTangent.log(x) + x * y + ADTangent.sin(y)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5000) tensor(2.2837)\n"
     ]
    }
   ],
   "source": [
    "# 使用 torch 验证\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.tensor(2.0), requires_grad=True)\n",
    "y = Variable(torch.tensor(5.0), requires_grad=True)\n",
    "z = torch.log(x) + x * y + torch.sin(y)\n",
    "z.backward()\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.后向微分\n",
    "后向自动微分关键步骤为：\n",
    "反向模式根据从后向前计算，依次得到对每个中间变量节点的偏导数，直到到达自变量节点处，这样就得到了每个输入的偏导数。在每个节点处，根据该节点的后续节点（前向传播中的后续节点）计算其导数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple, Callable, Dict, Optional\n",
    "\n",
    "_name = 1\n",
    "# 定义一个函数，用于生成新的名字\n",
    "def fresh_name():\n",
    "    # 声明一个全局变量，用于存储当前的名字\n",
    "    global _name\n",
    "    # 生成新的名字，格式为v加上当前的名字\n",
    "    name = f'v{_name}'\n",
    "    # 将当前的名字加1\n",
    "    _name += 1\n",
    "    # 返回新的名字\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Variable:\n",
    "    # 初始化函数，用于创建一个对象\n",
    "    def __init__(self, value, name=None):\n",
    "        # 将传入的value参数赋值给对象的value属性\n",
    "        self.value = value\n",
    "        # 如果传入的name参数为空，则调用fresh_name()函数生成一个新的名字，否则将传入的name参数赋值给对象的name属性\n",
    "        self.name = name or fresh_name()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # 返回self.value的字符串表示\n",
    "        return repr(self.value)\n",
    "\n",
    "    @staticmethod\n",
    "    # 定义一个函数，用于创建一个常量\n",
    "    def constant(value, name=None):\n",
    "        # 创建一个变量对象，传入值和名称\n",
    "        var = Variable(value, name)\n",
    "        # 打印变量的名称和值\n",
    "        print(f'{var.name} = {var.value}')\n",
    "        # 返回变量对象\n",
    "        return var\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return ops_mul(self, other)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return ops_add(self, other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return ops_sub(self, other)\n",
    "\n",
    "    def sin(self):\n",
    "        return ops_sin(self)\n",
    "    \n",
    "    def log(self):\n",
    "        return ops_log(self)\n",
    "\n",
    "def ops_mul(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value * other.value)\n",
    "    print(f'{x.name} = {self.name} * {other.name}')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        # 计算梯度\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dother = self # r=self.value * other.value\n",
    "        dx_self = other # dx_dother = self.value * other.value\n",
    "        dl_dself = dl_dx * dx_self\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        dl_inputs = [dl_dself, dl_dother]\n",
    "        return dl_inputs\n",
    "    # 记录操作的输入输出\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_add(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value + other.value)\n",
    "    print(f'{x.name} = {self.name} + {other.name}')\n",
    "\n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        # 计算梯度\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        return [dl_dself, dl_dother]\n",
    "    # 记录操作的输入输出\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_sub(self, other):\n",
    "    # forward\n",
    "    x = Variable(self.value - other.value)\n",
    "    print(f'{x.name} = {self.name} - {other.name}')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0)\n",
    "        dx_dother = Variable(-1.0)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        dl_dother = dl_dx * dx_dother\n",
    "        return [dl_dself, dl_dother]\n",
    "    # 记录操作的输入输出\n",
    "    tape = Tape(inputs=[self.name, other.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_sin(self):\n",
    "    # forward\n",
    "    x = Variable(np.sin(self.value))\n",
    "    print(f'{x.name} = sin({self.name})')\n",
    "\n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(np.cos(self.value))\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        return [dl_dself]\n",
    "    \n",
    "    # 记录操作的输入输出\n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "\n",
    "def ops_log(self):\n",
    "    # forward\n",
    "    x = Variable(np.log(self.value))\n",
    "    print(f'{x.name} = log({self.name})')\n",
    "    \n",
    "    # backward\n",
    "    def propagate(dl_doutputs):\n",
    "        dl_dx, = dl_doutputs\n",
    "        dx_dself = Variable(1.0 / self.value)\n",
    "        dl_dself = dl_dx * dx_dself\n",
    "        return [dl_dself]\n",
    "    # 记录操作的输入输出\n",
    "    tape = Tape(inputs=[self.name], outputs=[x.name], propagate=propagate)\n",
    "    gradient_tape.append(tape)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# 这个类的作用在于记录前向传播过程中产生的中间变量，以及这些中间变量之间的依赖关系，以便于在反向传播过程中计算梯度。\n",
    "# NamedTuple 是一个元组子类，用于定义不可变的数据结构。在这个类中，我们定义了三个属性：inputs、outputs 和 propagate。\n",
    "class Tape(NamedTuple):\n",
    "    # 输入列表\n",
    "    inputs: List[str]\n",
    "    # 输出列表\n",
    "    outputs: List[str]\n",
    "    # 传播函数，接受一个变量列表，返回一个变量列表\n",
    "    propagate : 'Callable[List[Variable], List[Variable]]'\n",
    "\n",
    "# 重置 Tape 的方法 reset_tape，方便运行多次自动微分，每次自动微分过程都会产生 Tape List\n",
    "gradient_tape : List[Tape] = []\n",
    "\n",
    "def reset_tape():\n",
    "    global _name\n",
    "    _name = 1\n",
    "    gradient_tape.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 = log(v-1)\n",
      "v2 = v-1 * v0\n",
      "v3 = v1 + v2\n",
      "v4 = sin(v0)\n",
      "v5 = v3 - v4\n",
      "np.float64(11.652071455223084)\n"
     ]
    }
   ],
   "source": [
    "def gred(l, results):\n",
    "    dl_d = {} # dl/dX 的所有梯度\n",
    "    dl_d[l.name] = Variable(1.)\n",
    "\n",
    "    print(\"dl_d\", dl_d)\n",
    "    # 添加 dl/dl = 1\n",
    "    \n",
    "    def gather_grads(entrys):\n",
    "        return [dl_d[entry] if entry in dl_d else None for entry in entrys]\n",
    "    \n",
    "\n",
    "    # 反向传播\n",
    "    for entry in reversed(gradient_tape):\n",
    "        # 计算梯度\n",
    "        print(entry)\n",
    "        dl_outputs = gather_grads(entry.outputs)\n",
    "        dl_inputs = entry.propagate(dl_outputs)\n",
    "        \n",
    "        for input, grad in zip(entry.inputs, dl_inputs):\n",
    "            if input not in dl_d:\n",
    "                dl_d[input] = grad\n",
    "            else:\n",
    "                dl_d[input] = dl_d[input] + grad\n",
    "                \n",
    "    for name, grad in dl_d.items():\n",
    "        print(f'd{l.name}_d{name} = {grad.value}')\n",
    "    \n",
    "    return gather_grads([result.name for result in results])\n",
    "\n",
    "reset_tape()\n",
    "x = Variable(2.0, name='v-1')\n",
    "y = Variable(5.0, name='v0')\n",
    "\n",
    "f = Variable.log(x) + x * y - Variable.sin(y)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_d {'v5': 1.0}\n",
      "Tape(inputs=['v3', 'v4'], outputs=['v5'], propagate=<function ops_sub.<locals>.propagate at 0x000001EC37302EE0>)\n",
      "v9 = v6 * v7\n",
      "v10 = v6 * v8\n",
      "Tape(inputs=['v0'], outputs=['v4'], propagate=<function ops_sin.<locals>.propagate at 0x000001EC37302790>)\n",
      "v12 = v10 * v11\n",
      "Tape(inputs=['v1', 'v2'], outputs=['v3'], propagate=<function ops_add.<locals>.propagate at 0x000001EC25759940>)\n",
      "v15 = v9 * v13\n",
      "v16 = v9 * v14\n",
      "Tape(inputs=['v-1', 'v0'], outputs=['v2'], propagate=<function ops_mul.<locals>.propagate at 0x000001EC372D4DC0>)\n",
      "v17 = v16 * v0\n",
      "v18 = v16 * v-1\n",
      "v19 = v12 + v18\n",
      "Tape(inputs=['v-1'], outputs=['v1'], propagate=<function ops_log.<locals>.propagate at 0x000001EC372D2EE0>)\n",
      "v21 = v15 * v20\n",
      "v22 = v17 + v21\n",
      "dv5_dv5 = 1.0\n",
      "dv5_dv3 = 1.0\n",
      "dv5_dv4 = -1.0\n",
      "dv5_dv0 = 1.7163378145367738\n",
      "dv5_dv1 = 1.0\n",
      "dv5_dv2 = 1.0\n",
      "dv5_dv-1 = 5.5\n",
      "5.5\n",
      "np.float64(1.7163378145367738)\n"
     ]
    }
   ],
   "source": [
    "dx, dy = gred(f, [x, y])\n",
    "print(dx)\n",
    "print(dy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
